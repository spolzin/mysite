{
  "hash": "f46ec8edc79bcf78f77ca096dd16e13a",
  "result": {
    "markdown": "---\nsidebar: true\n---\n\n\nI mapped every dollar store in Massachusetts alongside county level food insecurity rates. To collect these locations, I had to scrape each address from the store directories of the biggest dollar store chains in the country. I've shared a sample of the scraping code here.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(rvest) \nlibrary(tidyverse)\nlibrary(stringr)\n\n# Read in the URL and extract the HTML \nurl <- \"https://www.dollargeneral.com/store-directory\"\nhtml <- read_html(url) \n\n# Scrape the state links \ndiv_elements <- html %>%\n  html_nodes(\"div[class='state-list-item']\")\n\nstates <- div_elements %>%\n  html_nodes(\"a\") %>%\n  html_attr(\"href\")\n\n# Create a data frame for the state links \nstates_df <- data.frame(states) \nstates_df <- states_df %>% filter(states != \"/content/dollargeneral/en/store-directory/.html\")\n\n# Create an empty vector to store the city links \ncities <- c() \n\n# Loop through the state links \nfor (i in 1:length(states_df$states)) { \n  # Read in the URL and extract the HTML \n  url <- paste0(\"https://www.dollargeneral.com\", states_df$states[i])\n  html <- read_html(url) \n  \n  # Scrape the city links \n  div_elements <- html %>%\n    html_nodes(\"div[class='city-list-item']\")\n  \n  city <- div_elements %>%\n    html_nodes(\"a\") %>%\n    html_attr(\"href\")\n  \n  # Append the city links to the vector \n  cities <- c(cities, city) \n} \n\n# Create a data frame for the city links \ncities_df <- data.frame(cities)\n\n# Create a data frame for the addresses\naddresses <- data.frame()\n\nfor (i in 1:length(cities_df$cities)) {\n  # Read in the URL and extract the HTML \n  url <- paste0(\"https://www.dollargeneral.com\", cities_df$cities[i])\n\n  \n  # Extract elements with xpath\n  div_elements <- html %>% html_nodes('div[class=\"store__card\"]')\n  \n  # Check how many 'p' tags\n  num_p_tags <- div_elements %>% html_nodes(\"p\") %>% length()\n  \n  # Save each text element to a vector\n  text_vector <- vector()\n  for (i in 1:num_p_tags) {\n    text <- div_elements %>% html_nodes(\"p\") %>% .[[i]] %>% html_text()\n    text_vector <- c(text_vector, text)\n  }\n  \n  # Every third item, add to new row in a data frame\n  store_info <- data.frame(matrix(text_vector, ncol = 3, byrow = TRUE))\n  colnames(store_info) <- c(\"address\", \"city_state_zip\", \"phone\")\n  \n  # Append the store info to the complete data frame \n  addresses <- merge(addresses, store_info, all.x = T, all.y = T) \n}\n\naddresses[c(\"city\", \"state_zip\")] <- str_split_fixed(addresses$city_state_zip, \",\", 2)\naddresses$state_zip <- trimws(addresses$state_zip, \"l\")\naddresses[c(\"state\", \"zip\")] <- str_split_fixed(addresses$state_zip, \" \", 2)\naddresses <- addresses[-c(2, 5)]\naddresses <- addresses[,c(1,3,4,5,2)]\n\nwrite_delim(addresses, \"dollarGeneral.csv\", delim = \",\")\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}